\documentclass[12pt]{article}

%useful packages
\usepackage{color,soul}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{amsmath,amsthm,amscd,amssymb,bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=JungleGreen
}
\usepackage[utf8]{inputenc}
\usepackage[top=2cm, bottom=3cm, left=2cm, right=2cm]{geometry}
\usepackage{pgfplots}
\usepackage{enumitem}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{patterns}
\usepackage{tcolorbox}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{xcolor}

%personal definitions and commands
\newcommand{\R}{\mathbb{R}} 
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\e}{\epsilon}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} %allows numbering of single equations in align* environment
\newcommand{\mtx}[1]{\ensuremath{\bm{\mathit{#1}}}}
\newcommand{\B}{\hat{\boldsymbol{\beta}}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\N}{\mathcal{N}}



\title{ECON675 -- Assignment 3}
\author{Anirudh Yadav}
\setlength\parindent{0pt}
\begin{document}

\maketitle

\setcounter{tocdepth}{2}
\tableofcontents

\section{Non-linear least squares}

\subsection{Identifiability}
This is a standard M-estimation problem. The parameter vector $\mtx{\beta}_0$ is assumed to solve the population problem
\begin{align*}
\mtx{\beta}_0 = \arg \min_{\beta \in \R^d} \E[(y_i - \mu(\mtx{x}_i'\mtx{\beta}))^2].
\end{align*}
For $\mtx{\beta}_0$ to be identified, it must be the \textit{unique} solution to the above population problem (i.e. the unique minimizer). In math, this means for all $\e>0$ and for some $\delta >0$:
\begin{align*}
\sup_{|| \beta - \beta_0 || > \e} M(\mtx{\beta}) \geq M(\mtx{\beta}_0) + \delta
\end{align*}
where $M(\mtx{\beta}) = \E[(y_i - \mu(\mtx{x}_i'\mtx{\beta}))^2]$. Of course $\mtx{\beta}_0$ can be written in closed form if $\mu(\cdot)$ is linear. In this case, we know that 
\begin{align*}
\mtx{\beta}_0= \E[\mtx{x}_i\mtx{x}_i']^{-1}\E[\mtx{x}_iy_i].
\end{align*}

\subsection{Asymptotic normality}
The M-estimator is asymptotically normal if:
\begin{enumerate}
\item $\hat{\mtx{\beta}} \to_p \mtx{\beta}_0$
\item $\mtx{\beta}_0 \in int(B)$ and $m(\mtx{x}_i,\mtx{\beta}) \equiv (y_i - \mu(\mtx{x}_i'\mtx{\beta}))^2$ is 3 times continuously differentiable.
\item $\Sigma_0 = \V[\frac{\partial}{\partial \beta} m(\mtx{x}_i; \beta_0)] < \infty$ and $H_0 = \E[\frac{\partial^2}{\partial \beta \partial \beta'} m(\mtx{x}_i; \beta_0)]$ is full rank (and therefore invertible).
\end{enumerate}
Now, the FOC for the M-estimation problem is
\begin{align*}
0 &= \frac{1}{n} \sum_{i=1}^n (y_i-\mu(\mtx{x}_i'\mtx{\beta}))\dot{\mu}(\mtx{x}_i'\mtx{\beta}))\mtx{x}_i
\end{align*}
where $\dot \mu = \frac{\partial}{\partial \beta}  \mu (\mtx{x}_i'\mtx{\beta})$. So, we've converted the M-estimation problem into a Z-estimation problem. Then we can use the standard asymptotic normality result to arrive at a precise form of the asymptotic variance:
\begin{align*}
\sqrt{n}(\hat{\mtx{\beta}}-\mtx{\beta}_0) \to_d \N(0, H_0^{-1}\Sigma_0 H_0^{-1}).
\end{align*}
Now, taking the second derivative gives the Hessian
\begin{align*}
H_0 &= \E[\frac{\partial^2}{\partial \beta \partial \beta'} m(\mtx{x}_i; \beta_0)]\\
&=\E\left[-\dot{\mu}(\mtx{x}_i'\mtx{\beta}_0))\dot{\mu}(\mtx{x}_i'\mtx{\beta}_0))\mtx{x}_i\mtx{x}_i' + (y_i-\mu(\mtx{x}_i'\mtx{\beta}_0))\ddot{\mu}(\mtx{x}_i'\mtx{\beta}_0))\mtx{x}_i\mtx{x}_i'\right]\\
&=-\E[\dot{\mu}(\mtx{x}_i'\mtx{\beta}_0))^2\mtx{x}_i\mtx{x}_i']
\end{align*}
by LIE. And, the variance of the score is
\begin{align*}
\Sigma_0 &= \V[\frac{\partial}{\partial \beta} m(\mtx{x}_i; \beta_0)] \\
&=\E\left[(y_i-\mu(\mtx{x}_i'\mtx{\beta}_0))^2\dot{\mu}(\mtx{x}_i'\mtx{\beta}_0))^2\mtx{x}_i\mtx{x}_i'\right]\\
&=\E[\sigma^2(\mtx{x}_i)\dot{\mu}(\mtx{x}_i'\mtx{\beta}_0))^2\mtx{x}_i\mtx{x}_i']
\end{align*}
again by LIE. Then we have the asymptotic variance
\begin{align*}
\mtx{V}_0 = H_0^{-1}\Sigma_0 H_0^{-1}.
\end{align*}

\subsection{Variance estimator under heteroskedasticity}
Under heteroskedasticity we can use the sandwich variance estimator
\begin{align*}
\widehat{\mtx{V}}_{HC} &= \hat{H}^{-1}\hat \Sigma \hat{H}^{-1},
\end{align*}
where
\begin{align*}
\hat{H} &= \frac{1}{n} \sum_{i=1}^n \dot{\mu}(\mtx{x}_i'\hat{\mtx{\beta}}))^2\mtx{x}_i\mtx{x}_i'\\
\hat \Sigma &= \frac{1}{n} \sum_{i=1}^n \hat{e}^2_i\dot{\mu}(\mtx{x}_i'\hat{\mtx{\beta}}))^2\mtx{x}_i\mtx{x}_i'\\
\end{align*}
Now, to get an asymptotically valid CI for $|| \mtx{\beta}_0 ||^2$ we need to use the Delta Method. First, note that:
\begin{align*}
|| \mtx{\beta}_0 ||^2 &= \mtx{\beta}_0 ' \mtx{\beta}_0 \\
\implies \frac{\partial}{\partial \beta}|| \mtx{\beta}_0 ||^2 & = 2\mtx{\beta}_0
\end{align*}
Then, using the Delta Method
\begin{align*}
\sqrt{n}(||\hat{\mtx{\beta}}||^2-||\mtx{\beta}_0||^2) &\to_d 2\mtx{\beta}_0 \N(0,\mtx{V}_0)\\ 
&= \N(0,4 \mtx{\beta}_0'\mtx{V}_0 \mtx{\beta}_0)
\end{align*}
Thus, an asymptotically valid 95\% CI for $|| \mtx{\beta}_0 ||^2$ is
\begin{align*}
CI_{95} = \left[\hat{\mtx{\beta}} - 1.96 \sqrt{\frac{4\hat{\mtx{\beta}}'\widehat{\mtx{V}}_{HC}\hat{\mtx{\beta}}}{n}},\hat{\mtx{\beta}} + 1.96 \sqrt{\frac{4\hat{\mtx{\beta}}'\widehat{\mtx{V}}_{HC}\hat{\mtx{\beta}}}{n}}\right]
\end{align*}

\subsection{Variance estimator under homoskedasticity}
Using the above results, under homoskedasticity, the asymptotic variance collapses to
\begin{align*}
\mtx{V}_0 &= \E[\dot{\mu}(\mtx{x}_i'\mtx{\beta}_0))^2\mtx{x}_i\mtx{x}_i']^{-1} \sigma^2 \E[\dot{\mu}(\mtx{x}_i'\mtx{\beta}))^2\mtx{x}_i\mtx{x}_i'] \E[\dot{\mu}(\mtx{x}_i'\mtx{\beta}_0))^2\mtx{x}_i\mtx{x}_i']^{-1}\\
&=\sigma^2\E[\dot{\mu}(\mtx{x}_i'\mtx{\beta}_0))^2\mtx{x}_i\mtx{x}_i']^{-1}
\end{align*}
The variance estimator is now takes a simpler form
\begin{align*}
\widehat{\mtx{V}}_{HO} &= \hat{\sigma}^2 \hat{H}^{-1}
\end{align*}
where $\hat{H}$ is the same as above and
\begin{align*}
\hat{\sigma}^2 &= \frac{1}{n-d} \sum_{i=1}^n (y_i-\mu(\mtx{x}_i'\hat{\mtx{\beta}}))^2
\end{align*}
Then, as above, the asymptotically valid 95\% CI for $|| \mtx{\beta}_0 ||^2$ is
\begin{align*}
CI_{95} = \left[\hat{\mtx{\beta}} - 1.96 \sqrt{\frac{4\hat{\mtx{\beta}}'\widehat{\mtx{V}}_{HO}\hat{\mtx{\beta}}}{n}},\hat{\mtx{\beta}} + 1.96 \sqrt{\frac{4\hat{\mtx{\beta}}'\widehat{\mtx{V}}_{HO}\hat{\mtx{\beta}}}{n}}\right].
\end{align*}






\end{document}
