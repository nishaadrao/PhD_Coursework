\documentclass[12pt]{article}

%useful packages
\usepackage{color,soul}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{amsmath,amsthm,amscd,amssymb,bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=JungleGreen
}
\usepackage[utf8]{inputenc}
\usepackage[top=2cm, bottom=3cm, left=2cm, right=2cm]{geometry}
\usepackage{pgfplots}
\usepackage{enumitem}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{patterns}
\usepackage{tcolorbox}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{xcolor}

%personal definitions and commands
\newcommand{\R}{\mathbb{R}} 
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\e}{\epsilon}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} %allows numbering of single equations in align* environment
\newcommand{\mtx}[1]{\ensuremath{\bm{\mathit{#1}}}}
\newcommand{\B}{\hat{\boldsymbol{\beta}}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\N}{\mathcal{N}}



\title{ECON675 -- Assignment 5}
\author{Anirudh Yadav}
\setlength\parindent{0pt}
\begin{document}

\maketitle

\setcounter{tocdepth}{2}
\tableofcontents

\newpage

\section{Many instruments asymptotics}

\subsection{Some moments}
First,
\begin{align*}
\E[\mtx{u}'\mtx{u}/n] &= \frac{1}{n} \E[\mtx{u}'\mtx{u}]= \frac{1}{n} \sum_{i=1}^n \E[u_i^2] = \sigma^2_u.
\end{align*}
An analogous derivation shows that $\E[\mtx{v}'\mtx{v}/n] = \sigma^2_v$. \\

Next,
\begin{align*}
\E[\mtx{x}'\mtx{u}/n]= \frac{1}{n} \E[\mtx{x}'\mtx{u}]&=  \frac{1}{n} \E[(\mtx{\pi}'\mtx{Z}'+\mtx{v}')\mtx{u}]\\
&=\frac{1}{n}\mtx{\pi}'\mtx{Z}'\E[\mtx{u}]+\frac{1}{n}\E[\mtx{v}'\mtx{u}]\\
&= \frac{1}{n} \sum_{i=1}^n \E[v_iu_i]\\
&= \sigma^2_{uv},
\end{align*}
where I used the assumptions that $\mtx{Z}$ and $\mtx{\pi}$ are nonrandom and $\E[\mtx{u}] = \bm{0}$.\\

Now,
\begin{align*}
\E[\mtx{x}'\mtx{P}\mtx{u}/n] &= \frac{1}{n}\E[(\mtx{\pi}'\mtx{Z}'+\mtx{v}')\mtx{P}\mtx{u}]\\
&=\frac{1}{n}\E[\mtx{\pi}'\mtx{Z}'\mtx{P}\mtx{u}]+\frac{1}{n}\E[\mtx{v}'\mtx{P}\mtx{u}]\\
&=\frac{1}{n}\E[\mtx{\pi}'\mtx{Z}'\mtx{u}]+\frac{1}{n}\E[\mtx{v}'\mtx{P}\mtx{u}]\\
&=\frac{1}{n}\E[\mtx{v}'\mtx{P}\mtx{u}]\\
&= \frac{K}{n}\sigma^2_{uv}
\end{align*}
since $\E[v_iu_j] = 0$ for all $i\neq j$ and $\sum_{i=1}^n P_{ii} = K$. An analogous derivation proves the last result $\E[\mtx{u}'\mtx{P}\mtx{u}/n] = K/n \sigma^2_u$.

\subsection{Some probability limits}
First,
\begin{align*}
\mtx{x}'\mtx{x}/n &= (\mtx{\pi}'\mtx{Z}'+\mtx{v}')(\mtx{Z}\mtx{\pi}+\mtx{v})/n\\
&= \frac{\mtx{\pi}'\mtx{Z}'\mtx{Z}\mtx{\pi}}{n} + \frac{\mtx{\pi}'\mtx{Z}'\mtx{v}}{n} + \frac{\mtx{v}'\mtx{Z}\mtx{\pi}}{n} + \frac{\mtx{v}'\mtx{v}}{n}\\
&\to_p \mu + \E[\mtx{\pi}'\mtx{z}_iv_i] + \E[\mtx{z}_i'\mtx{\pi}v_i] + \E[v_i^2]\\
&=\mu + \sigma^2_v
\end{align*}
Next,
\begin{align*}
\mtx{x}'\mtx{P}\mtx{x}/n &= (\mtx{\pi}'\mtx{Z}'+\mtx{v}')\mtx{P}(\mtx{Z}\mtx{\pi}+\mtx{v})/n\\
&=\frac{\mtx{\pi}'\mtx{Z}'\mtx{Z}\mtx{\pi}}{n} + \frac{\mtx{\pi}'\mtx{Z}'\mtx{v}}{n} + \frac{\mtx{v}'\mtx{Z}\mtx{\pi}}{n} + \frac{\mtx{v}'\mtx{P}\mtx{v}}{n}\\
&\to_p \mu + \rho \sigma^2_v.
\end{align*}
The above convergence result involves a few steps, which I've suppressed for brevity. First, it uses the assumption that $\mtx{Z}$ and $\mtx{\pi}$ are nonrandom. More importantly, it uses the result that
\begin{align*}
 \frac{\mtx{v}'\mtx{P}\mtx{v}}{n} \to_p \E[{\mtx{v}'\mtx{P}\mtx{v}}/{n}] = \rho\sigma^2_v
\end{align*} 


since $K/n \to \rho$. Note that this is not just a direct application of the WLLN, since we're not dealing with a sum of iid random variables. Rather, you can show that $\V[{\mtx{v}'\mtx{P}\mtx{v}}/{n}]$ is bounded in probability (i.e. it goes to zero at some rate), and then use the Markov/Chebyshev inequality to get the desired convergence result. This type of result will be used a lot in the following questions too.\\

An analogous derivation proves the last result, $\mtx{x}'\mtx{P}\mtx{u}/n \to_p \rho \sigma^2_u$.

\subsection{plim of the classical 2SLS estimator}
The classical 2SLS estimator is
\begin{align*}
\hat\beta_{\texttt{2SLS}} &= (\mtx{x}'\mtx{P}\mtx{x})^{-1}(\mtx{x}'\mtx{P}\mtx{y})\\
&= (\mtx{x}'\mtx{P}\mtx{x})^{-1}\mtx{x}'\mtx{P}(\mtx{x}\mtx{\beta} + \mtx{u})\\
&= \beta + (\mtx{x}'\mtx{P}\mtx{x})^{-1}(\mtx{x}'\mtx{P}\mtx{u})\\
&= \beta + (\mtx{x}'\mtx{P}\mtx{x}/n)^{-1}(\mtx{x}'\mtx{P}\mtx{u}/n)\\
&\to_p \beta + \frac{\rho\sigma^2_u}{ \mu + \rho \sigma^2_v},
\end{align*}
using the CMT and the above results. Thus, $\hat\beta_{\texttt{2SLS}} = \beta + \frac{\rho\sigma^2_u}{ \mu + \rho \sigma^2_v} + o_p(1)$.

\subsection{plim of the bias-corrected 2SLS estimator}
The bias-corrected 2SLS estimator is
\begin{align*}
\hat\beta_{\texttt{2SLS}} &= (\mtx{x}'\check{\mtx{P}}\mtx{x})^{-1}(\mtx{x}'\check{\mtx{P}}\mtx{y})\\
&=\beta + (\mtx{x}'\check{\mtx{P}}\mtx{x}/n)^{-1}(\mtx{x}'\check{\mtx{P}}\mtx{u}/n)
\end{align*}
Now,
\begin{align*}
\mtx{x}'\check{\mtx{P}}\mtx{u}/n &=\frac{1}{n}(\mtx{\pi}'\mtx{Z}'+\mtx{v}')(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u}\\
&=\frac{\mtx{\pi}'\mtx{Z}'\mtx{u}}{n} -\frac{\frac{K}{n}\mtx{\pi}'\mtx{Z}'\mtx{u}}{n} + \frac{\mtx{v}'\mtx{P}\mtx{u}}{n} - \frac{\frac{K}{n}\mtx{v}'\mtx{u}}{n}\\
&\to_p 0 - 0 + \rho \sigma_{uv}^2 + \rho \sigma^2_{uv}\\
&=0.
\end{align*}
Thus, $\hat\beta_{\texttt{2SLS}} \to_p \beta$.

\subsection{Asymptotic normality of the bias-corrected 2SLS estimator}

\subsubsection{}
First note that
\begin{align*}
\mtx{x}'\check{\mtx{P}}\mtx{u} &=(\mtx{\pi}'\mtx{Z}'+\mtx{v}')(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u}\\
&=\mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u} + \mtx{v}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u}\\
&=\mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u} + \left(\check{\mtx{v}}' + \frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'\right)(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u}\\
&=\mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u} + \check{\mtx{v}}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u} + \frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u},
\end{align*}
as required.

\subsubsection{}
Next, note that
\begin{align*}
\E[\mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u}] &= \mtx{\pi}'\mtx{Z}'\E[\mtx{u}] - \frac{K}{n}\mtx{\pi}'\mtx{Z}'\E[\mtx{u}] = 0,
\end{align*}
since $\mtx{Z}$ is nonrandom. Accordingly, the CLT implies that
\begin{align*}
\frac{1}{\sqrt{n}} \mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u} \to_d \N(0, V_1(\rho)),
\end{align*}
where
\begin{align*}
V_1(\rho) &= \lim_{n\to \infty} \V[1/\sqrt{n}\mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u}]\\
&=\lim_{n\to \infty} \frac{1}{n}\E[\mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u}\mtx{u}' (\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{Z}\mtx{\pi}]\\
&=\lim_{n\to \infty} \frac{1}{n} \sigma^2_u \left[ \mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{Z}\mtx{\pi}\right]\\
&=\lim_{n\to \infty} \frac{1}{n} \sigma^2_u \left[ \mtx{\pi}'\mtx{Z}'\mtx{Z}\mtx{\pi}- 2\frac{K}{n}\mtx{\pi}'\mtx{Z}'\mtx{Z}\mtx{\pi} + \frac{K^2}{n^2}\mtx{Z}\mtx{\pi}'\mtx{Z}'\mtx{Z}\mtx{\pi}\right]\\
&= \sigma^2_u (1-\rho^2).
\end{align*}

\subsubsection{}
Now,
\begin{align*}
\E[ \check{\mtx{v}}'(\mtx{P} -K/n\mtx{I}_n) \mtx{u}] &= \E\left[\left({\mtx{v}}' - \frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'\right)\mtx{P}\mtx{u} - \frac{K}{n} \left({\mtx{v}}' - \frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'\right)\mtx{u}\right]\\
&=\E[{\mtx{v}}'\mtx{P}\mtx{u}] - \frac{\sigma^2_{uv}}{\sigma^2_u}\E[\mtx{u}'\mtx{P}\mtx{u}] - \frac{K}{n}\E[\mtx{v}'\mtx{u}] + \frac{K}{n}\frac{\sigma^2_{uv}}{\sigma^2_u}\E[\mtx{u}'\mtx{u}]
\end{align*}
Then, plugging in the results from part 1 gives
\begin{align*}
\E[ \check{\mtx{v}}'(\mtx{P} -K/n\mtx{I}_n) \mtx{u}] &= K\sigma^2_{uv} - \frac{\sigma^2_{uv}}{\sigma^2_u} K \sigma^2_u - \frac{K}{n} \cdot n \sigma^2_{uv} + \frac{K}{n}\frac{\sigma^2_{uv}}{\sigma^2_u} \cdot n \sigma^2_u = 0,
\end{align*}
as required. \\

To get the convergence result we would do the following. Compute $\V[\check{\mtx{v}}'(\mtx{P} -K/n\mtx{I}_n) \mtx{u}]$. Using the assumption $\V[\mtx{u}|\check{\mtx{v}}] = \sigma^2_u\mtx{I}_n$, it can be shown that
\begin{align*}
\lim_{n\to \infty} \V[\check{\mtx{v}}'(\mtx{P} -K/n\mtx{I}_n) \mtx{u}] = O(K).
\end{align*}
Then, we can somehow use the Markov inequality to get the desired convergence result.

\iffalse
Now, to show the convergence result note that
\begin{align*}
 \check{\mtx{v}}'(\mtx{P} -K/n\mtx{I}_n) \mtx{u} &= {\mtx{v}}'\mtx{P}\mtx{u} -  \frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'\mtx{P}\mtx{u} - \frac{K}{n}\mtx{v}'\mtx{u} + \frac{K}{n}\frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'\mtx{u}
\end{align*}
Thus
\begin{align*}
\frac{1}{\sqrt{K}} \check{\mtx{v}}'(\mtx{P} -K/n\mtx{I}_n) \mtx{u} =  {\mtx{v}}'\mtx{P}\mtx{u} -  \frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'\mtx{P}\mtx{u} - \frac{K}{n}\mtx{v}'\mtx{u} + \frac{K}{n}\frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'\mtx{u}
\end{align*}
\fi

\subsubsection{}
Analogous derivations to the above question give the desired results.

\subsubsection{}
Now,
\begin{align*}
\E[\mtx{x}'\check{\mtx{P}}\mtx{u}] &= \E[(\mtx{\pi}'\mtx{Z}'+\mtx{v}')(\mtx{P} -K/n\mtx{I}_n)\mtx{u}]\\
&= \E[\mtx{\pi}'\mtx{Z}'(\mtx{P} -K/n\mtx{I}_n)\mtx{u}] + \E[\mtx{v}'(\mtx{P} -K/n\mtx{I}_n)\mtx{u}]\\
&=0 + \E[\mtx{v}'\mtx{P}\mtx{u}] - K/n\E[\mtx{v}'\mtx{u}]\\
&= K \sigma^2_{uv} - K/n \cdot n \sigma^2_{uv}\\
&=0.
\end{align*}
And
\begin{align*}
\vartheta^2 = \V[\mtx{x}'\check{\mtx{P}}\mtx{u}/\sqrt{n}] &= \frac{1}{n}\E[\mtx{x}'\check{\mtx{P}}\mtx{u}\mtx{u}'\check{\mtx{P}}\mtx{x}]\\
&=\frac{1}{n}\E[\mtx{x}'(\mtx{P} -K/n\mtx{I}_n)\mtx{u}\mtx{u}'(\mtx{P} -K/n\mtx{I}_n)\mtx{x}]\\
&=\frac{1}{n}\E[(\mtx{x}'\mtx{P}\mtx{u} - K/n\mtx{x}'\mtx{u})(\mtx{u}'\mtx{P}\mtx{x}-K/n\mtx{u}'\mtx{x})]
\end{align*}

\subsubsection{}
Note that
\begin{align*}
\sqrt{n}(\hat\beta_{\texttt{2SLS}} - \beta) &= (\mtx{x}'\check{\mtx{P}}\mtx{x}/n)^{-1}(\frac{1}{\sqrt{n}}\mtx{x}'\check{\mtx{P}}\mtx{u})
\end{align*}
And we assume that
\begin{align*}
\frac{1}{\sqrt{n}}\mtx{x}'\check{\mtx{P}}\mtx{u} \to_d \N(0, \vartheta^2)
\end{align*}
Thus,
\begin{align*}
\sqrt{n}(\hat\beta_{\texttt{2SLS}} - \beta) \to_d \N(0, \E[\mtx{x}'\check{\mtx{P}}\mtx{x}]^{-1}\vartheta^2\E[\mtx{x}'\check{\mtx{P}}\mtx{x}]^{-1})
\end{align*}
Intuitively, I think that when $K/n \to \rho = 0$, then the many instruments problem dissipates, so that the bias-corrected 2SLS estimator and the classical 2SLS estimator are asymptotically equivalent.

\end{document}
