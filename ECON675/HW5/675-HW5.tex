\documentclass[12pt]{article}

%useful packages
\usepackage{color,soul}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{amsmath,amsthm,amscd,amssymb,bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=JungleGreen
}
\usepackage[utf8]{inputenc}
\usepackage[top=2cm, bottom=3cm, left=2cm, right=2cm]{geometry}
\usepackage{pgfplots}
\usepackage{enumitem}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{patterns}
\usepackage{tcolorbox}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{xcolor}

% Packages to create table 1
\usepackage{pdflscape}
\usepackage{subfig}
\usepackage{graphicx}

%personal definitions and commands
\newcommand{\R}{\mathbb{R}} 
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\e}{\epsilon}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} %allows numbering of single equations in align* environment
\newcommand{\mtx}[1]{\ensuremath{\bm{\mathit{#1}}}}
\newcommand{\B}{\hat{\boldsymbol{\beta}}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\N}{\mathcal{N}}



\title{ECON675 -- Assignment 5}
\author{Anirudh Yadav}
\setlength\parindent{0pt}
\begin{document}

\maketitle

\setcounter{tocdepth}{2}
\tableofcontents

\newpage

\section{Many instruments asymptotics}

\subsection{Some moments}
First,
\begin{align*}
\E[\mtx{u}'\mtx{u}/n] &= \frac{1}{n} \E[\mtx{u}'\mtx{u}]= \frac{1}{n} \sum_{i=1}^n \E[u_i^2] = \sigma^2_u.
\end{align*}
An analogous derivation shows that $\E[\mtx{v}'\mtx{v}/n] = \sigma^2_v$. \\

Next,
\begin{align*}
\E[\mtx{x}'\mtx{u}/n]= \frac{1}{n} \E[\mtx{x}'\mtx{u}]&=  \frac{1}{n} \E[(\mtx{\pi}'\mtx{Z}'+\mtx{v}')\mtx{u}]\\
&=\frac{1}{n}\mtx{\pi}'\mtx{Z}'\E[\mtx{u}]+\frac{1}{n}\E[\mtx{v}'\mtx{u}]\\
&= \frac{1}{n} \sum_{i=1}^n \E[v_iu_i]\\
&= \sigma^2_{uv},
\end{align*}
where I used the assumptions that $\mtx{Z}$ and $\mtx{\pi}$ are nonrandom and $\E[\mtx{u}] = \bm{0}$.\\

Now,
\begin{align*}
\E[\mtx{x}'\mtx{P}\mtx{u}/n] &= \frac{1}{n}\E[(\mtx{\pi}'\mtx{Z}'+\mtx{v}')\mtx{P}\mtx{u}]\\
&=\frac{1}{n}\E[\mtx{\pi}'\mtx{Z}'\mtx{P}\mtx{u}]+\frac{1}{n}\E[\mtx{v}'\mtx{P}\mtx{u}]\\
&=\frac{1}{n}\E[\mtx{\pi}'\mtx{Z}'\mtx{u}]+\frac{1}{n}\E[\mtx{v}'\mtx{P}\mtx{u}]\\
&=\frac{1}{n}\E[\mtx{v}'\mtx{P}\mtx{u}]\\
&= \frac{K}{n}\sigma^2_{uv}
\end{align*}
since $\E[v_iu_j] = 0$ for all $i\neq j$ and $\sum_{i=1}^n P_{ii} = K$. An analogous derivation proves the last result $\E[\mtx{u}'\mtx{P}\mtx{u}/n] = K/n \sigma^2_u$.

\subsection{Some probability limits}
First,
\begin{align*}
\mtx{x}'\mtx{x}/n &= (\mtx{\pi}'\mtx{Z}'+\mtx{v}')(\mtx{Z}\mtx{\pi}+\mtx{v})/n\\
&= \frac{\mtx{\pi}'\mtx{Z}'\mtx{Z}\mtx{\pi}}{n} + \frac{\mtx{\pi}'\mtx{Z}'\mtx{v}}{n} + \frac{\mtx{v}'\mtx{Z}\mtx{\pi}}{n} + \frac{\mtx{v}'\mtx{v}}{n}\\
&\to_p \mu + \E[\mtx{\pi}'\mtx{z}_iv_i] + \E[\mtx{z}_i'\mtx{\pi}v_i] + \E[v_i^2]\\
&=\mu + \sigma^2_v
\end{align*}
Next,
\begin{align*}
\mtx{x}'\mtx{P}\mtx{x}/n &= (\mtx{\pi}'\mtx{Z}'+\mtx{v}')\mtx{P}(\mtx{Z}\mtx{\pi}+\mtx{v})/n\\
&=\frac{\mtx{\pi}'\mtx{Z}'\mtx{Z}\mtx{\pi}}{n} + \frac{\mtx{\pi}'\mtx{Z}'\mtx{v}}{n} + \frac{\mtx{v}'\mtx{Z}\mtx{\pi}}{n} + \frac{\mtx{v}'\mtx{P}\mtx{v}}{n}\\
&\to_p \mu + \rho \sigma^2_v.
\end{align*}
The above convergence result involves a few steps, which I've suppressed for brevity. First, it uses the assumption that $\mtx{Z}$ and $\mtx{\pi}$ are nonrandom. More importantly, it uses the result that
\begin{align*}
 \frac{\mtx{v}'\mtx{P}\mtx{v}}{n} \to_p \E[{\mtx{v}'\mtx{P}\mtx{v}}/{n}] = \rho\sigma^2_v
\end{align*} 


since $K/n \to \rho$. Note that this is not just a direct application of the WLLN, since we're not dealing with a sum of iid random variables. Rather, you can show that $\V[{\mtx{v}'\mtx{P}\mtx{v}}/{n}]$ is bounded in probability (i.e. it goes to zero at some rate), and then use the Markov/Chebyshev inequality to get the desired convergence result. This type of result will be used a lot in the following questions too.\\

An analogous derivation proves the last result, $\mtx{x}'\mtx{P}\mtx{u}/n \to_p \rho \sigma^2_u$.

\subsection{plim of the classical 2SLS estimator}
The classical 2SLS estimator is
\begin{align*}
\hat\beta_{\texttt{2SLS}} &= (\mtx{x}'\mtx{P}\mtx{x})^{-1}(\mtx{x}'\mtx{P}\mtx{y})\\
&= (\mtx{x}'\mtx{P}\mtx{x})^{-1}\mtx{x}'\mtx{P}(\mtx{x}\mtx{\beta} + \mtx{u})\\
&= \beta + (\mtx{x}'\mtx{P}\mtx{x})^{-1}(\mtx{x}'\mtx{P}\mtx{u})\\
&= \beta + (\mtx{x}'\mtx{P}\mtx{x}/n)^{-1}(\mtx{x}'\mtx{P}\mtx{u}/n)\\
&\to_p \beta + \frac{\rho\sigma^2_u}{ \mu + \rho \sigma^2_v},
\end{align*}
using the CMT and the above results. Thus, $\hat\beta_{\texttt{2SLS}} = \beta + \frac{\rho\sigma^2_u}{ \mu + \rho \sigma^2_v} + o_p(1)$.

\subsection{plim of the bias-corrected 2SLS estimator}
The bias-corrected 2SLS estimator is
\begin{align*}
\hat\beta_{\texttt{2SLS}} &= (\mtx{x}'\check{\mtx{P}}\mtx{x})^{-1}(\mtx{x}'\check{\mtx{P}}\mtx{y})\\
&=\beta + (\mtx{x}'\check{\mtx{P}}\mtx{x}/n)^{-1}(\mtx{x}'\check{\mtx{P}}\mtx{u}/n)
\end{align*}
Now,
\begin{align*}
\mtx{x}'\check{\mtx{P}}\mtx{u}/n &=\frac{1}{n}(\mtx{\pi}'\mtx{Z}'+\mtx{v}')(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u}\\
&=\frac{\mtx{\pi}'\mtx{Z}'\mtx{u}}{n} -\frac{\frac{K}{n}\mtx{\pi}'\mtx{Z}'\mtx{u}}{n} + \frac{\mtx{v}'\mtx{P}\mtx{u}}{n} - \frac{\frac{K}{n}\mtx{v}'\mtx{u}}{n}\\
&\to_p 0 - 0 + \rho \sigma_{uv}^2 + \rho \sigma^2_{uv}\\
&=0.
\end{align*}
Thus, $\hat\beta_{\texttt{2SLS}} \to_p \beta$.

\subsection{Asymptotic normality of the bias-corrected 2SLS estimator}

\subsubsection{}
First note that
\begin{align*}
\mtx{x}'\check{\mtx{P}}\mtx{u} &=(\mtx{\pi}'\mtx{Z}'+\mtx{v}')(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u}\\
&=\mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u} + \mtx{v}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u}\\
&=\mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u} + \left(\check{\mtx{v}}' + \frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'\right)(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u}\\
&=\mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u} + \check{\mtx{v}}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u} + \frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u},
\end{align*}
as required.

\subsubsection{}
Next, note that
\begin{align*}
\E[\mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u}] &= \mtx{\pi}'\mtx{Z}'\E[\mtx{u}] - \frac{K}{n}\mtx{\pi}'\mtx{Z}'\E[\mtx{u}] = 0,
\end{align*}
since $\mtx{Z}$ is nonrandom. Accordingly, the CLT implies that
\begin{align*}
\frac{1}{\sqrt{n}} \mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u} \to_d \N(0, V_1(\rho)),
\end{align*}
where
\begin{align*}
V_1(\rho) &= \lim_{n\to \infty} \V[1/\sqrt{n}\mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u}]\\
&=\lim_{n\to \infty} \frac{1}{n}\E[\mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{u}\mtx{u}' (\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{Z}\mtx{\pi}]\\
&=\lim_{n\to \infty} \frac{1}{n} \sigma^2_u \left[ \mtx{\pi}'\mtx{Z}'(\mtx{P} - \frac{K}{n}\mtx{I}_n)(\mtx{P} - \frac{K}{n}\mtx{I}_n)\mtx{Z}\mtx{\pi}\right]\\
&=\lim_{n\to \infty} \frac{1}{n} \sigma^2_u \left[ \mtx{\pi}'\mtx{Z}'\mtx{Z}\mtx{\pi}- 2\frac{K}{n}\mtx{\pi}'\mtx{Z}'\mtx{Z}\mtx{\pi} + \frac{K^2}{n^2}\mtx{Z}\mtx{\pi}'\mtx{Z}'\mtx{Z}\mtx{\pi}\right]\\
&= \sigma^2_u (1-\rho^2).
\end{align*}

\subsubsection{}
Now,
\begin{align*}
\E[ \check{\mtx{v}}'(\mtx{P} -K/n\mtx{I}_n) \mtx{u}] &= \E\left[\left({\mtx{v}}' - \frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'\right)\mtx{P}\mtx{u} - \frac{K}{n} \left({\mtx{v}}' - \frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'\right)\mtx{u}\right]\\
&=\E[{\mtx{v}}'\mtx{P}\mtx{u}] - \frac{\sigma^2_{uv}}{\sigma^2_u}\E[\mtx{u}'\mtx{P}\mtx{u}] - \frac{K}{n}\E[\mtx{v}'\mtx{u}] + \frac{K}{n}\frac{\sigma^2_{uv}}{\sigma^2_u}\E[\mtx{u}'\mtx{u}]
\end{align*}
Then, plugging in the results from part 1 gives
\begin{align*}
\E[ \check{\mtx{v}}'(\mtx{P} -K/n\mtx{I}_n) \mtx{u}] &= K\sigma^2_{uv} - \frac{\sigma^2_{uv}}{\sigma^2_u} K \sigma^2_u - \frac{K}{n} \cdot n \sigma^2_{uv} + \frac{K}{n}\frac{\sigma^2_{uv}}{\sigma^2_u} \cdot n \sigma^2_u = 0,
\end{align*}
as required. \\

To get the convergence result we would do the following. Compute $\V[\check{\mtx{v}}'(\mtx{P} -K/n\mtx{I}_n) \mtx{u}]$. Using the assumption $\V[\mtx{u}|\check{\mtx{v}}] = \sigma^2_u\mtx{I}_n$, it can be shown that
\begin{align*}
\lim_{n\to \infty} \V[\check{\mtx{v}}'(\mtx{P} -K/n\mtx{I}_n) \mtx{u}] = O(K).
\end{align*}
Then, we can somehow use the Markov inequality to get the desired convergence result.

\iffalse
Now, to show the convergence result note that
\begin{align*}
 \check{\mtx{v}}'(\mtx{P} -K/n\mtx{I}_n) \mtx{u} &= {\mtx{v}}'\mtx{P}\mtx{u} -  \frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'\mtx{P}\mtx{u} - \frac{K}{n}\mtx{v}'\mtx{u} + \frac{K}{n}\frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'\mtx{u}
\end{align*}
Thus
\begin{align*}
\frac{1}{\sqrt{K}} \check{\mtx{v}}'(\mtx{P} -K/n\mtx{I}_n) \mtx{u} =  {\mtx{v}}'\mtx{P}\mtx{u} -  \frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'\mtx{P}\mtx{u} - \frac{K}{n}\mtx{v}'\mtx{u} + \frac{K}{n}\frac{\sigma^2_{uv}}{\sigma^2_u}\mtx{u}'\mtx{u}
\end{align*}
\fi

\subsubsection{}
Analogous derivations to the above question give the desired results.

\subsubsection{}
Now,
\begin{align*}
\E[\mtx{x}'\check{\mtx{P}}\mtx{u}] &= \E[(\mtx{\pi}'\mtx{Z}'+\mtx{v}')(\mtx{P} -K/n\mtx{I}_n)\mtx{u}]\\
&= \E[\mtx{\pi}'\mtx{Z}'(\mtx{P} -K/n\mtx{I}_n)\mtx{u}] + \E[\mtx{v}'(\mtx{P} -K/n\mtx{I}_n)\mtx{u}]\\
&=0 + \E[\mtx{v}'\mtx{P}\mtx{u}] - K/n\E[\mtx{v}'\mtx{u}]\\
&= K \sigma^2_{uv} - K/n \cdot n \sigma^2_{uv}\\
&=0.
\end{align*}
And
\begin{align*}
\vartheta^2 = \V[\mtx{x}'\check{\mtx{P}}\mtx{u}/\sqrt{n}] &= \frac{1}{n}\E[\mtx{x}'\check{\mtx{P}}\mtx{u}\mtx{u}'\check{\mtx{P}}\mtx{x}]\\
&=\frac{1}{n}\E[\mtx{x}'(\mtx{P} -K/n\mtx{I}_n)\mtx{u}\mtx{u}'(\mtx{P} -K/n\mtx{I}_n)\mtx{x}]\\
&=\frac{1}{n}\E[(\mtx{x}'\mtx{P}\mtx{u} - K/n\mtx{x}'\mtx{u})(\mtx{u}'\mtx{P}\mtx{x}-K/n\mtx{u}'\mtx{x})]
\end{align*}

\subsubsection{}
Note that
\begin{align*}
\sqrt{n}(\hat\beta_{\texttt{2SLS}} - \beta) &= (\mtx{x}'\check{\mtx{P}}\mtx{x}/n)^{-1}(\frac{1}{\sqrt{n}}\mtx{x}'\check{\mtx{P}}\mtx{u})
\end{align*}
And we assume that
\begin{align*}
\frac{1}{\sqrt{n}}\mtx{x}'\check{\mtx{P}}\mtx{u} \to_d \N(0, \vartheta^2)
\end{align*}
Thus,
\begin{align*}
\sqrt{n}(\hat\beta_{\texttt{2SLS}} - \beta) \to_d \N(0, \E[\mtx{x}'\check{\mtx{P}}\mtx{x}]^{-1}\vartheta^2\E[\mtx{x}'\check{\mtx{P}}\mtx{x}]^{-1})
\end{align*}
Intuitively, I think that when $K/n \to \rho = 0$, then the many instruments problem dissipates, so that the bias-corrected 2SLS estimator and the classical 2SLS estimator are asymptotically equivalent.

\newpage

\section{Weak instruments -- simulations}
Table 1 (overleaf) presents the simulation results, which are are a nice illustration of the weak instruments problem. The following results are worth noting:
\begin{itemize}
\item For all values of $\gamma$, the OLS estimators of $\beta$ are very bad: recall that the true value of $\beta$ is zero and for literally every one of the 20,000 OLS regressions, we reject the null that $\beta=0$ at the 95\% level! 
This is unsurprising, given that $x_i$ is endogenous.
\item For lower values of $\gamma$ (i.e. when $z_i$ is a ``weak'' instrument for $x_i$) the 2SLS estimators of $\beta$ are also very bad. For higher values of $\gamma$, the 2SLS estimator is clearly consistent for $\beta$; and for $\gamma = \sqrt{99/n}$, the 2SLS estimator is very precise.
\end{itemize}


\begin{landscape}
\begin{table}[!htp]
\centering
\renewcommand{\arraystretch}{1.5}  
\caption{Weak Instrument Summary Statistics}\label{tab:weak_IV_simulation}
\resizebox{0.45\paperwidth}{0.021\paperwidth}{\subfloat[][$\gamma^2=0/n$ ($F\approx1$)]{\input{tab0.txt}}}
\resizebox{0.45\paperwidth}{0.021\paperwidth}{\subfloat[][$\gamma^2=0.25/n$ ($F\approx1.25$)]{\input{tab1.txt}}}\\
\resizebox{0.45\paperwidth}{0.021\paperwidth}{\subfloat[][$\gamma^2=9/n$ ($F\approx10$)]{\input{tab2.txt}}}
\resizebox{0.45\paperwidth}{0.021\paperwidth}{\subfloat[][$\gamma^2=99/n$ ($F\approx100$)]{\input{tab3.txt}}}\\
\end{table}
\end{landscape}

\newpage

\section{Weak instruments -- empirical studies}

\subsection{Angrist and Krueger (1991)}
\begin{table}[htpb!]
\centering
\caption{OLS and 2SLS Estimates of Return to Schooling}
\begin{tabular}{l*{4}{c}}
\hline
            &\multicolumn{1}{c}{(1)}&\multicolumn{1}{c}{(2)}&\multicolumn{1}{c}{(3)}&\multicolumn{1}{c}{(4)}\\
            &\multicolumn{1}{c}{l\_w\_wage}&\multicolumn{1}{c}{l\_w\_wage}&\multicolumn{1}{c}{l\_w\_wage}&\multicolumn{1}{c}{l\_w\_wage}\\
\hline
educ        &      0.0632&      0.0632&      0.0806&      0.0600\\
            &  (0.000377)&  (0.000377)&    (0.0164)&    (0.0290)\\
[1em]
non\_white   &      -0.257&      -0.257&      -0.230&      -0.263\\
            &   (0.00459)&   (0.00459)&    (0.0261)&    (0.0458)\\
[1em]
married     &       0.248&       0.248&       0.244&       0.249\\
            &   (0.00358)&   (0.00358)&   (0.00487)&   (0.00726)\\
[1em]
SMSA        &      -0.176&      -0.176&      -0.158&      -0.180\\
            &   (0.00305)&   (0.00305)&    (0.0174)&    (0.0305)\\
[1em]
age\_q       &            &     -0.0760&            &     -0.0741\\
            &            &    (0.0601)&            &    (0.0626)\\
[1em]
age\_sq      &            &    0.000770&            &    0.000743\\
            &            &  (0.000667)&            &  (0.000712)\\
\hline
\(N\)       &      329509&      329509&      329509&      329509\\
\hline
\multicolumn{5}{l}{\footnotesize Standard errors in parentheses}\\
\end{tabular}
\end{table}

Table 2 replicates columns (5), (7), (6) and (8) from Angrist and Krueger (1991, AK) as required. The OLS and 2SLS estimates are quite similar, suggesting that there is little bias in the OLS estimates. From these models, a reasonable estimate of the return to school is around 0.06 (i.e. each additional year of schooling increases wages by around 6\%, on average).\\

Bound, Jaeger and Baker (1995, BJB) outline a number of potential problems with these estimates. First, the association between quarter of birth and years of schooling is very weak (so we're in weak instruments territory). Furthermore, quarter of birth may affect wages through channels other than its affect on educational attainment (`other seasonal effects' on wages). BJB argue that the weak association between educational attainment and quarter of birth indicates that even if other seasonal effects are weak, they could still have large effects on the estimated return to schooling.\newpage

\subsection{Bound, Jaeger and Baker (1995)}

Table 3, below, replicates the first two columns of Table 3 in BJB. As BJB note, it is striking that these results look very similar to AK's results, even though the simulated instruments contain no information about educational attainment. The estimated standard deviations are also pretty close to the estimated standard errors from AK's 2SLS regressions. These results imply that AK's 2SLS results suffer badly from the weak instruments problem; in particular, they show that when the correlation between the instruments and the endogenous variable is small, then even very large sample sizes do not guarantee that quantitatively important finite sample bias will be eliminated from 2SLS estimates.

\begin{table}[htpb!]
\centering
\caption{2SLS Estimates of Return to Schooling Using Permuted Quarter of Birth*}
\begin{tabular}{llc}
\hline
            &\multicolumn{1}{c}{(1)}&\multicolumn{1}{c}{(2)} \\
            &\multicolumn{1}{c}{l\_w\_wage}&\multicolumn{1}{c}{l\_w\_wage}\\
\hline
Mean        &      .0646165 &  .0646411       \\
Std. dev. & .0387673 &    .0387972\\
\hline
\multicolumn{3}{l}{\footnotesize *500 replications}\\
\end{tabular}
\end{table}

\newpage

\section{Appendix}
\scriptsize
\subsection{\texttt{R} code}
\subsubsection{Question 2}
\begin{verbatim}
## ECON675: ASSIGNMENT 5
## Q2: WEAK INSTRUMENTS SIMULATIONS
## Anirudh Yadav 
## 11/19/2018

######################################################################
# Load packages, clear workspace
######################################################################
rm(list = ls())             #clear workspace
library(foreach)            #for looping
library(data.table)         #for data manipulation
library(Matrix)             #fast matrix calcs
library(ggplot2)            #for pretty plots
library(sandwich)           #for variance-covariance estimation 
library(xtable)             #for latex tables
library(boot)               #for bootstrapping
library(mvtnorm)            #for MVN stuff
library(AER)                #for IV regressions
options(scipen = 999)       #forces R to use normal numbers instead of scientific notation

######################################################################
# Generate random data for each simulation
######################################################################
N     = 200
M     = 5000
SIGMA = matrix(c(1,0,0,0,1,0.99,0,0.99,1),3,3)

set.seed(1234)

# Generate Z, U, V
W     = replicate(M,rmvnorm(N, mean = c(0,0,0), sigma = SIGMA, method="chol"))

# Get Y (assuming that beta=0, Y=U)
Y     = W[,2,]

# Generate X matrix for each value of gamma
gamma.vec = sqrt((1/N)*c(0,0.25,9,99))
X         = lapply(1:length(gamma.vec),function(i) gamma.vec[i]*W[,1,]+W[,3,])

######################################################################
# Compute OLS statistics for each gamma, and simulation
######################################################################

# Run OLS for each gamma and each simulation -- this spits out 5000 lm's for each gamma
ols.big = foreach(j=1:length(gamma.vec)) %do%
  lapply(1:M, function(i) lm(Y[,i]~X[[j]][,i]-1))

# Extract point estimates, standard errors, t-stats
ols.beta = foreach(j=1:length(gamma.vec)) %do%
  sapply(1:M, function(i) ols.big[[j]][[i]]$coefficients)

ols.se   = foreach(j=1:length(gamma.vec)) %do%
  sapply(1:M, function(i) coef(summary(ols.big[[j]][[i]]))[,"Std. Error"])

ols.t    = sapply(1:length(gamma.vec),function(j) ols.beta[[j]]/ols.se[[j]])

ols.rej  = ifelse(ols.t>1.96,1,0)

# Compute desired summary statistics across the simulations (spits out a list containing results for each gamma)
ols.results = foreach(j=1:length(gamma.vec)) %do%
  rbind(c(mean(ols.beta[[j]]),sd(ols.beta[[j]]),quantile(ols.beta[[j]], probs = c(0.1, 0.5 ,0.9))),
  c(mean(ols.se[[j]]),sd(ols.se[[j]]),quantile(ols.se[[j]], probs = c(0.1, 0.5 ,0.9))),
  c(mean(ols.rej[,j]),sd(ols.rej[,j]),quantile(ols.rej[,j], probs = c(0.1, 0.5 ,0.9))))

# Remove big objects!
rm(ols.big,ols.beta,ols.se,ols.t,ols.rej)

######################################################################
# Compute 2SLS statistics for each gamma, and simulation
######################################################################

# Run 2SLS for each gamma and each simulation -- this spits out 5000 ivreg's for each gamma
# WATCH OUT: this takes a minute or so!
iv.big = foreach(j=1:length(gamma.vec)) %do%
  lapply(1:M, function(i) ivreg(Y[,i]~X[[j]][,i]-1|W[,1,i]))

# Extract point estimates, standard errors, t-stats
iv.beta = foreach(j=1:length(gamma.vec)) %do%
  sapply(1:M, function(i) iv.big[[j]][[i]]$coefficients)

iv.se   = foreach(j=1:length(gamma.vec)) %do%
  sapply(1:M, function(i) summary(iv.big[[j]][[i]])[["coefficients"]][,"Std. Error"])

iv.t    = sapply(1:length(gamma.vec),function(j) iv.beta[[j]]/iv.se[[j]])

iv.rej  = ifelse(iv.t>1.96,1,0)

# Run first-stage regression and extract F-statistics
iv.f       = foreach(j=1:length(gamma.vec)) %do%
  sapply(1:M, function(i) summary(lm(X[[j]][,i]~W[,1,i]-1))$fstatistic[1])

# Combine results for each gamma
iv.results = foreach(j=1:length(gamma.vec)) %do%
  rbind(c(mean(iv.beta[[j]]),sd(iv.beta[[j]]),quantile(iv.beta[[j]], probs = c(0.1, 0.5 ,0.9))),
        c(mean(iv.se[[j]]),sd(iv.se[[j]]),quantile(iv.se[[j]], probs = c(0.1, 0.5 ,0.9))),
        c(mean(iv.rej[,j]),sd(iv.rej[,j]),quantile(iv.rej[,j], probs = c(0.1, 0.5 ,0.9))),
        c(mean(iv.f[[j]]),sd(iv.f[[j]]),quantile(iv.f[[j]], probs = c(0.1, 0.5 ,0.9))))

# Remove big objects!
rm(iv.big,iv.beta,iv.se,iv.t,iv.rej,iv.f)
\end{verbatim}

\subsubsection{Question 3}
\begin{verbatim}
## ECON675: ASSIGNMENT 5
## Q3: WEAK INSTRUMENTS -- EMPIRICAL STUDIES
## Anirudh Yadav 
## 11/19/2018

######################################################################
# Load packages, clear workspace
######################################################################
rm(list = ls())             #clear workspace
library(foreach)            #for looping
library(data.table)         #for data manipulation
library(Matrix)             #fast matrix calcs
library(ggplot2)            #for pretty plots
library(sandwich)           #for variance-covariance estimation 
library(xtable)             #for latex tables
library(boot)               #for bootstrapping
library(mvtnorm)            #for MVN stuff
library(AER)                #for IV regressions
options(scipen = 999)       #forces R to use normal numbers instead of scientific notation

######################################################################
# Input data
###################################################################### 
ak <- fread('PhD_Coursework/ECON675/HW5/Angrist_Krueger.csv')


######################################################################
# [3.1] Angrist-Krueger
######################################################################

# make YOB dummies
ak[, .N, "YoB_ld"]
for(year_i in unique(ak$YoB_ld)){
  
  ak[ ,temp := 0]
  ak[YoB_ld == year_i ,temp := 1]
  setnames(ak, "temp", paste0("d_YOB_ld_", year_i))
  
}
# get a list of all year dummies but one. Exclude the proper one to match coeffs 
year_dummies <- setdiff(grep("d_YOB", colnames(ak), value = TRUE), "d_YOB_ld_0")

# make QoB dummies 
for(qob_i in unique(ak$QoB)){
  
  ak[ ,temp := 0]
  ak[QoB == qob_i ,temp := 1]
  setnames(ak, "temp", paste0("d_QoB_", qob_i))
  
}

# get qob dummy list.  Exclude the proper one to match coeffs 
qob_dummies <- setdiff(grep("d_QoB", colnames(ak), value = TRUE), "d_QoB_1")

# make cross variables of year dummies and qob
#note there is almost certainly a better way to do this but here we are 
inter_list <- NULL
for(d_year in year_dummies){
  
  for(d_qob in qob_dummies){
    
    ak[, temp := get(d_qob)*get(d_year)]
    setnames(ak, "temp", paste0(d_year, "X", d_qob))
    inter_list<- c(inter_list, paste0(d_year, "X", d_qob))
  }
}


# standard controls 
# (i) race, (ii) marrital status, (iii) SMSA, (iv) dummies for
# region, and (iv) dummies for YoB ld.
std_cont <- c("non_white","married", "SMSA", 
              "ENOCENT","ESOCENT", "MIDATL", 
              "MT", "NEWENG", "SOATL", "WNOCENT",
              "WSOCENT",  year_dummies) # get year dummies but leave one out 

# save extra controls 
extra_cont <- c("age_q", "age_sq")

#================#
# ==== ols 1 ====
#================#

# make the formula 
ols1_form <- as.formula(paste0("l_w_wage~educ +", paste(std_cont, collapse = " + ")))

# run ols 
out_ols1 <- data.table(tidy(lm(ols1_form, data = ak)))

# keep what I need 
out_ols1 <- out_ols1[term %chin% c("educ"), c("term", "estimate", "std.error")]
out_ols1[, model := "OLS 1"]

#================#
# ==== OlS 2 ====
#================#

# make the formula 
ols2_form <- as.formula(paste0("l_w_wage~educ +", paste(std_cont, collapse = " + "), " + ", paste0(extra_cont, collapse = " + ")))

#run ols
out_ols2 <- data.table(tidy(lm(ols2_form, data = ak)))

# keep what I need 
out_ols2 <- out_ols2[term %chin% c("educ"), c("term", "estimate", "std.error")]
out_ols2[, model := "OLS 2"]

#===============#
# ==== 2sls ====
#===============#

wrap_2sls <- function(in_data){
  
  #=================#
  # ==== 2sls 1 ====
  #=================#
  
  iv_form <- as.formula(paste0("l_w_wage~educ +", paste(std_cont, collapse = " + "), 
                               "| ", 
                               paste(std_cont, collapse = " + "), " + ", paste0(inter_list, collapse = " + ")))
  iv_reg1 <- data.table(tidy(ivreg(iv_form , data = in_data)))
  
  # keep what I need 
  iv_reg1 <- iv_reg1[term %chin% c("educ"), c("term", "estimate", "std.error")]
  iv_reg1[, model := "2sls 1"]
  
  #=================#
  # ==== 2sls 2 ====
  #=================#
  
  
  iv_form2 <- as.formula(paste0("l_w_wage~educ +", paste(std_cont, collapse = " + "), "+", paste0(extra_cont, collapse = " + "),
                                "| ", 
                                paste(std_cont, collapse = " + "), 
                                " + ", paste0(inter_list, collapse = " + "), 
                                "+", paste0(extra_cont, collapse = " + ")))
  iv_reg2 <- data.table(tidy(ivreg(iv_form2 , data = in_data)))
  
  # keep what I need 
  iv_reg2 <- iv_reg2[term %chin% c("educ"), c("term", "estimate", "std.error")]
  iv_reg2[, model := "2sls 2"]
  
  # stack 2sls 
  out_2sls <- rbind(iv_reg1, iv_reg2)
  
  return(out_2sls)
  
}#end 2sls function 

# run function 
ak_2sls <- wrap_2sls(ak)

#========================#
# ==== output tables ====
#========================#

output_3.1 <- rbind(out_ols1, out_ols2, ak_2sls)
setcolorder(output_3.1, c("model", "term", "estimate", "std.error"))



#================#
# ==== Q 3.2 ====
#================#


#===================================#
# ==== Fast 2sls function ==========
#===================================#

fast_2sls <- function(in_data){
  
  #===============#
  # ==== reg1 ====
  #===============#
  
  # make x z and y matrices
  y <- as.matrix(in_data[, l_w_wage])
  x <- as.matrix(in_data[, educ])
  cont <- as.matrix(in_data[, c( std_cont, 'const'), with = FALSE])
  z <- as.matrix(in_data[, c(inter_list, std_cont, "const"), with = FALSE])
  
  first_stage_fit <-  z%*%Matrix::solve(Matrix::crossprod(z))%*%(Matrix::crossprod(z, x))
  
  # make x' matrix 
  x_prime <- cbind(first_stage_fit, cont)
  
  
  form_2nd <-  Matrix::solve(Matrix::crossprod(x_prime))%*%(Matrix::crossprod(x_prime, y))
  
  reg1 <- data.table( term = "educ", estimate = form_2nd[1,1], model = "2sls 1")
  
  #===============#
  # ==== reg2 ====
  #===============#
  
  cont <- as.matrix(in_data[, c( std_cont, extra_cont, 'const'), with = FALSE])
  z <- as.matrix(in_data[, c(inter_list, std_cont, extra_cont, "const"), with = FALSE])
  
  first_stage_fit <-  z%*%Matrix::solve(Matrix::crossprod(z))%*%(Matrix::crossprod(z, x))
  
  # make x' matrix 
  x_prime <- cbind(first_stage_fit, cont)
  
  
  form_2nd <-  Matrix::solve(Matrix::crossprod(x_prime))%*%(Matrix::crossprod(x_prime, y))
  
  reg2 <- data.table( term = "educ", estimate = form_2nd[1,1], model = "2sls 2")
  
  # stack results and retur n
  out_results <- rbind(reg1, reg2)
}


#=========================#
# ==== run simulation ====
#=========================#

# copy data for permutation 
ak_perm <- copy(ak)

# add constant 
ak_perm[, const := 1]

# write a function 
sim_warper <- function(sim_i, in_data = ak_perm ){
  
  # get random sampel 
  perm <- sample(c(1:nrow(in_data)))
  
  # purmute data 
  in_data[, QoB := QoB[perm]]
  
  # clear out dummy variables 
  in_data <- in_data[, -c(grep("d_QoB", colnames(in_data), value = TRUE), inter_list), with = FALSE]
  
  # redo dummy vars 
  for(qob_i in unique(in_data$QoB)){
    
    in_data[ ,temp := 0]
    in_data[QoB == qob_i ,temp := 1]
    setnames(in_data, "temp", paste0("d_QoB_", qob_i))
    
  }
  # recalculate interactions
  inter_list <- NULL
  for(d_year in year_dummies){
    
    for(d_qob in qob_dummies){
      
      in_data[, temp := get(d_qob)*get(d_year)]
      setnames(in_data, "temp", paste0(d_year, "X", d_qob))
      inter_list<- c(inter_list, paste0(d_year, "X", d_qob))
    }
  }
  
  # run 2sls funciton on new data 
  ak_2sls_i <- fast_2sls(in_data)
  
  # add simulation 
  ak_2sls_i[, sim := sim_i]
  
  # retunrn it 
  return(ak_2sls_i)
  
} # end funciton 

# run simulations in parallel
output_list <- foreach(sim = 1 : 5000,
                       .inorder = FALSE,
                       .packages = "data.table",
                       .options.multicore = list(preschedule = FALSE, cleanup = 9)) %dopar% sim_warper(sim_i = sim)

# stop clusters 
stopCluster(cl)



#==========================#
# ==== organize output ====
#==========================#

# stack data 
sim_res3.2 <- rbindlist(output_list)

# make table 
output3.2 <- sim_res3.2[, list(mean = mean(estimate), std.dev = sd(estimate)), "model"]

\end{verbatim}

\subsection{\texttt{STATA} code}

\subsubsection{Question 2}
\begin{verbatim}
clear all
set more off
cap log close

program define weak_IV, rclass
    syntax [, obs(integer 200) f_stat(real 10) ]
	drop _all
	
	set obs `obs'
	
	* DGP
	gen u = rnormal()
	gen v = 0.99 * u + sqrt(1-0.99^2) * rnormal()
	gen z = rnormal()
	
	local gamma_0 = sqrt((`f_stat' - 1) / `obs')
	gen x = `gamma_0' * z + v
	gen y = u
	
	* OLS
	qui reg y x, robust
	return scalar OLS_b   = _b[x]
	return scalar OLS_se  = _se[x]
	return scalar OLS_rej = abs(_b[x]/_se[x]) > 1.96
	
	* 2SLS
	qui ivregress 2sls y (x = z)
	return scalar TSLS_b   = _b[x]
	return scalar TSLS_se  = _se[x]
	return scalar TSLS_rej = abs(_b[x]/_se[x]) > 1.96
	qui reg x z
	return scalar TSLS_F   = e(F)
end

* simulation 1: F = 1 
simulate OLS_b=r(OLS_b) OLS_se=r(OLS_se) OLS_rej=r(OLS_rej) ///
    TSLS_b=r(TSLS_b) TSLS_se=r(TSLS_se) TSLS_rej=r(TSLS_rej) TSLS_F=r(TSLS_F), ///
    reps(5000) seed(123) nodots: ///
    weak_IV, f_stat(1)
	
local k = 1
matrix Results = J(7, 5, .)

qui sum OLS_b, detail
matrix Results[`k',1] = r(mean)
matrix Results[`k',2] = r(sd)
matrix Results[`k',3] = r(p10)
matrix Results[`k',4] = r(p50)
matrix Results[`k',5] = r(p90)
local k = `k' + 1

qui sum OLS_se, detail
matrix Results[`k',1] = r(mean)
matrix Results[`k',2] = r(sd)
matrix Results[`k',3] = r(p10)
matrix Results[`k',4] = r(p50)
matrix Results[`k',5] = r(p90)
local k = `k' + 1

qui sum OLS_rej, detail
matrix Results[`k',1] = r(mean)
matrix Results[`k',2] = r(sd)
matrix Results[`k',3] = r(p10)
matrix Results[`k',4] = r(p50)
matrix Results[`k',5] = r(p90)
local k = `k' + 1

qui sum TSLS_b, detail
matrix Results[`k',1] = r(mean)
matrix Results[`k',2] = r(sd)
matrix Results[`k',3] = r(p10)
matrix Results[`k',4] = r(p50)
matrix Results[`k',5] = r(p90)
local k = `k' + 1

qui sum TSLS_se, detail
matrix Results[`k',1] = r(mean)
matrix Results[`k',2] = r(sd)
matrix Results[`k',3] = r(p10)
matrix Results[`k',4] = r(p50)
matrix Results[`k',5] = r(p90)
local k = `k' + 1

qui sum TSLS_rej, detail
matrix Results[`k',1] = r(mean)
matrix Results[`k',2] = r(sd)
matrix Results[`k',3] = r(p10)
matrix Results[`k',4] = r(p50)
matrix Results[`k',5] = r(p90)
local k = `k' + 1

qui sum TSLS_F, detail
matrix Results[`k',1] = r(mean)
matrix Results[`k',2] = r(sd)
matrix Results[`k',3] = r(p10)
matrix Results[`k',4] = r(p50)
matrix Results[`k',5] = r(p90)
local k = `k' + 1

mat2txt, matrix(Results) saving(result1.txt) format(%9.4f) replace


\end{verbatim}

\subsubsection{Question 3}
\begin{verbatim}
********************************************************************************
* ECON675: ASSIGNMENT 5
* Q3: WEAK INSTRUMENTS -- EMPIRICAL STUDIES
* Anirudh Yadav
* 11/19/2018
********************************************************************************


********************************************************************************
* Preliminaries
********************************************************************************
clear all
set more off

* Set working directory 
global dir "/Users/Anirudh/Desktop/GitHub"


********************************************************************************
* Import AK data
********************************************************************************

use "$dir/PhD_Coursework/ECON675/HW5/Angrist_Krueger.dta"

********************************************************************************
* [3.1] Run AK regressions
********************************************************************************
eststo ols1: reg l_w_wage educ non_white married SMSA i.region i.YoB_ld, r
eststo ols2: reg l_w_wage educ non_white married SMSA i.region i.YoB_ld age_q age_sq, r
eststo  iv1: ivregress 2sls l_w_wage non_white married SMSA i.region i.YoB_ld (educ = i.QoB##i.YoB_ld),r
eststo  iv2: ivregress 2sls l_w_wage non_white married SMSA i.region i.YoB_ld age_q age_sq (educ = i.QoB##i.YoB_ld), r

esttab ols1 ols2 iv1 iv2 using "$dir/PhD_Coursework/ECON675/HW5/q3_ak_results.tex", keep(educ non_white SMSA married age_q age_sq) se nostar

********************************************************************************
* [3.2] Run BJB permutation regressions
********************************************************************************
capture program drop IV_quick
program define IV_quick, rclass
    syntax varlist(max=1) [, model(integer 1) ]
	local x "`varlist'"
	
	if (`model' == 1) {
	    capture drop educ_hat
		qui reg educ non_white married SMSA i.region i.YoB_ld i.YoB_ld##i.`x'
		predict educ_hat
		qui reg l_w_wage educ_hat non_white married SMSA i.region i.YoB_ld
		return scalar beta = _b[educ_hat]
	}
	if (`model' == 2) {
	    capture drop educ_hat
		qui reg educ non_white married SMSA age_q age_sq i.region i.YoB_ld i.YoB_ld##i.`x'
		predict educ_hat
		qui reg l_w_wage educ_hat non_white married SMSA age_q age_sq i.region i.YoB_ld
		return scalar beta = _b[educ_hat]
	}
end


permute QoB TSLS_1_b = r(beta), reps(500) seed(123) saving("$dir/PhD_Coursework/ECON675/HW5/premute1.dta", replace): ///
    IV_quick QoB, model(1)
	
permute QoB TSLS_2_b = _b[educ], reps(500) seed(123) saving("$dir/PhD_Coursework/ECON675/HW5/premute2.dta", replace): ///
    IV_quick QoV, model(2)

clear all
use "$dir/PhD_Coursework/ECON675/HW5/premute1.dta"
sum TSLS_1_b

clear all
use "$dir/PhD_Coursework/ECON675/HW5/premute2.dta"
sum TSLS_2_b
\end{verbatim}




\end{document}
